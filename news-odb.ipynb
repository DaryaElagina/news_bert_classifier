{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# News","metadata":{}},{"cell_type":"markdown","source":"### Мы решили торговать на бирже, причем так, чтобы решение о покупке акций принимала нейросеть на основе последних новостей о той или иной компании. Для этого нужно научиться классифицировать все новости. Дана база новостей из разных источников news.csv. Необходимо написать модель, классифицирующую новости по источникам.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport torch\nimport transformers \nfrom tqdm import notebook \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:14:54.570885Z","iopub.execute_input":"2021-12-13T14:14:54.571318Z","iopub.status.idle":"2021-12-13T14:14:54.577185Z","shell.execute_reply.started":"2021-12-13T14:14:54.571274Z","shell.execute_reply":"2021-12-13T14:14:54.575950Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/news-opendb/news (1).csv')\ndisplay(data.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:14:59.465450Z","iopub.execute_input":"2021-12-13T14:14:59.466529Z","iopub.status.idle":"2021-12-13T14:15:01.747119Z","shell.execute_reply.started":"2021-12-13T14:14:59.466476Z","shell.execute_reply":"2021-12-13T14:15:01.746278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.info()\ndata['source'].value_counts()\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:15:07.600875Z","iopub.execute_input":"2021-12-13T14:15:07.601249Z","iopub.status.idle":"2021-12-13T14:15:07.654345Z","shell.execute_reply.started":"2021-12-13T14:15:07.601208Z","shell.execute_reply":"2021-12-13T14:15:07.653304Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_drop = data.dropna()\ndata_drop = data_drop.head(12000)\ndata_drop.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:15:36.478839Z","iopub.execute_input":"2021-12-13T14:15:36.479198Z","iopub.status.idle":"2021-12-13T14:15:36.504590Z","shell.execute_reply.started":"2021-12-13T14:15:36.479160Z","shell.execute_reply":"2021-12-13T14:15:36.503648Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.BertTokenizer(\n    vocab_file='../input/deeppavlov-rubertbasecased/vocab.txt')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:15:46.632322Z","iopub.execute_input":"2021-12-13T14:15:46.632635Z","iopub.status.idle":"2021-12-13T14:15:47.026998Z","shell.execute_reply.started":"2021-12-13T14:15:46.632600Z","shell.execute_reply":"2021-12-13T14:15:47.026168Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Сначала сделаем предсказание для данных, где источник новостей известен, тем самым натренируем модель.","metadata":{}},{"cell_type":"code","source":"cut_data = data_drop['news'].apply(lambda x: x[:512])\nprint(cut_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:16:09.343044Z","iopub.execute_input":"2021-12-13T14:16:09.343639Z","iopub.status.idle":"2021-12-13T14:16:09.364752Z","shell.execute_reply.started":"2021-12-13T14:16:09.343590Z","shell.execute_reply":"2021-12-13T14:16:09.364094Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Чтобы токенизировать все новости, применим метод apply():\ntokenized = cut_data.apply(\n  lambda x: tokenizer.encode(x, add_special_tokens=True)) \n\nprint(tokenized)\n\n#Применим метод padding, чтобы после токенизации длины исходных текстов в корпусе были равными. Только при таком условии будет работать модель BERT. Пусть стандартной длиной вектора n будет длина наибольшего во всём датасете вектора. \n#Остальные векторы дополним нулями.\n#Теперь поясним модели, что нули не несут значимой информации. Это нужно для компоненты модели, которая называется attention. Отбросим эти токены и «создадим маску» для действительно важных токенов, то есть укажем нулевые и не нулевые значения\n\nmax_len = 0\nfor i in tokenized.values:\n     if len(i) > max_len:\n        max_len = len(i)\nprint(max_len)        \n#tokenized_values = tokenized.values[:, :512]\npadded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n\nattention_mask = np.where(padded != 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:16:25.599761Z","iopub.execute_input":"2021-12-13T14:16:25.600543Z","iopub.status.idle":"2021-12-13T14:16:53.306975Z","shell.execute_reply.started":"2021-12-13T14:16:25.600493Z","shell.execute_reply":"2021-12-13T14:16:53.306161Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(tokenized.values)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:17:29.846923Z","iopub.execute_input":"2021-12-13T14:17:29.847974Z","iopub.status.idle":"2021-12-13T14:17:29.854713Z","shell.execute_reply.started":"2021-12-13T14:17:29.847908Z","shell.execute_reply":"2021-12-13T14:17:29.853597Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(padded.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:19:31.605974Z","iopub.execute_input":"2021-12-13T14:19:31.606895Z","iopub.status.idle":"2021-12-13T14:19:31.612061Z","shell.execute_reply.started":"2021-12-13T14:19:31.606850Z","shell.execute_reply":"2021-12-13T14:19:31.611169Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Инициализируем конфигурацию BertConfig (англ. Bert Configuration). \n#В качестве аргумента передадим ей JSON-файл с описанием настроек модели. \nconfig = transformers.BertConfig.from_json_file(\n    '../input/deeppavlov-rubertbasecased/config.json')\nmodel = transformers.BertModel.from_pretrained(\n    '../input/deeppavlov-rubertbasecased/pytorch_model.bin', config=config) ","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:19:41.086944Z","iopub.execute_input":"2021-12-13T14:19:41.087337Z","iopub.status.idle":"2021-12-13T14:19:50.803234Z","shell.execute_reply.started":"2021-12-13T14:19:41.087294Z","shell.execute_reply":"2021-12-13T14:19:50.802402Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(attention_mask.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:19:56.421138Z","iopub.execute_input":"2021-12-13T14:19:56.421485Z","iopub.status.idle":"2021-12-13T14:19:56.427609Z","shell.execute_reply.started":"2021-12-13T14:19:56.421453Z","shell.execute_reply":"2021-12-13T14:19:56.426171Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Начнём преобразование текстов в эмбеддинги.\nЭмбеддинги модель BERT создаёт батчами. Чтобы хватило оперативной памяти, сделаем размер батча небольшим (например, 100).","metadata":{}},{"cell_type":"code","source":"batch_size = 100 \n# сделаем пустой список для хранения эмбеддингов твитов\nembeddings = []\n# Сделаем цикл по батчам. Отображать прогресс будет функция notebook():\nfor i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n    # преобразуем данные в формат тензоров (англ. tensor) — многомерных векторов в библиотеке torch\n    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n    # преобразуем маску\n    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n# Для ускорения вычисления функцией no_grad() в библиотеке torch укажем, \n# что градиенты не нужны: модель BERT обучать не будем.\n    with torch.no_grad():\n        # Чтобы получить эмбеддинги для батча, передадим модели данные и маску:\n        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n# Из полученного тензора извлечём нужные элементы и добавим в список всех эмбеддингов:\n# преобразуем элементы методом numpy() к типу numpy.array\n    embeddings.append(batch_embeddings[0][:,0,:].numpy())","metadata":{"execution":{"iopub.status.busy":"2021-12-13T14:20:30.622313Z","iopub.execute_input":"2021-12-13T14:20:30.622638Z","iopub.status.idle":"2021-12-13T16:02:35.981521Z","shell.execute_reply.started":"2021-12-13T14:20:30.622606Z","shell.execute_reply":"2021-12-13T16:02:35.980144Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Обучим модель логистической регрессии на эмбеддингах.\nДля корректного тестирования поделим их на обучающую и тестовую выборки в соотношении 50:50.","metadata":{}},{"cell_type":"code","source":"#Соберём все эмбеддинги в матрицу признаков вызовов функции concatenate():\nfeatures = np.concatenate(embeddings)\ntarget = data_drop['source']\nfeatures_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=12345)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T16:02:50.827164Z","iopub.execute_input":"2021-12-13T16:02:50.827513Z","iopub.status.idle":"2021-12-13T16:02:50.924692Z","shell.execute_reply.started":"2021-12-13T16:02:50.827477Z","shell.execute_reply":"2021-12-13T16:02:50.923456Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#print(features_train.shape)\n#print(target_train.shape)\n#print(features_test.shape)\n#print(features_test.head(10))\n#print(target_test.shape)\n#print(target_test.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T16:02:54.945597Z","iopub.execute_input":"2021-12-13T16:02:54.946973Z","iopub.status.idle":"2021-12-13T16:02:54.951424Z","shell.execute_reply.started":"2021-12-13T16:02:54.946914Z","shell.execute_reply":"2021-12-13T16:02:54.950349Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Логистическая регрессия\nmodel_regression = LogisticRegression(random_state=12345, solver='liblinear') # инициализируем модель LogisticRegression\nmodel_regression.fit(features_train, target_train) # обучаем модель на тренировочной выборке\nresult_regression = model_regression.score(features_test, target_test) # считаем качество модели на тестовой выборке\npredictions = model_regression.predict(features_test)\nprint(result_regression)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T16:02:58.285401Z","iopub.execute_input":"2021-12-13T16:02:58.286043Z","iopub.status.idle":"2021-12-13T16:03:06.159037Z","shell.execute_reply.started":"2021-12-13T16:02:58.285996Z","shell.execute_reply":"2021-12-13T16:03:06.156214Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Сделаем предсказание для данных, где источник новостей неизвестен, с помощью ранее натренированной модели.","metadata":{}},{"cell_type":"code","source":"data_for_predict = data[data['source'].isnull()]\nprint(data_for_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cut_data_for_predict = data_for_predict['news'].apply(lambda x: x[:512])\nprint(cut_data_for_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized = cut_data_for_predict.apply(\n  lambda x: tokenizer.encode(x, add_special_tokens=True)) \n\nprint(tokenized)\n\nmax_len = 0\nfor i in tokenized.values:\n     if len(i) > max_len:\n        max_len = len(i)\nprint(max_len)        \n\npadded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n\nattention_mask = np.where(padded != 0, 1, 0)\n\nprint(padded.shape)\nprint(attention_mask.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 100 \n\nembeddings = []\n\nfor i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n    \n    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n    \n    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n\n    with torch.no_grad():\n      \n        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n\n    embeddings.append(batch_embeddings[0][:,0,:].numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_new = np.concatenate(embeddings)\n\nresult = model_regression.predict(features_new)\n\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(result).to_csv(\"predict.csv\", index = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}